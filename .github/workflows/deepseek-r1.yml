name: Ollama Deepseek R1 Workflow

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: 'Text prompt to send to Deepseek model'
        required: true
        type: string

jobs:
  run-ollama:
    runs-on: ubuntu-latest
    outputs:
      model_response: ${{ steps.query-model.outputs.response }}
    
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      # Set up Docker layer caching to speed up builds
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Cache Docker images between runs
      - name: Cache Docker images
        uses: actions/cache@v3
        with:
          path: /tmp/docker-cache
          key: ${{ runner.os }}-docker-${{ hashFiles('**/deepseek-r1.yml') }}
          restore-keys: |
            ${{ runner.os }}-docker-

      # Cache Ollama models between runs
      - name: Cache Ollama models
        uses: actions/cache@v3
        with:
          path: ~/.ollama/models
          key: ${{ runner.os }}-ollama-deepseek-coder
          restore-keys: |
            ${{ runner.os }}-ollama-

      # Save Docker image to cache and load it
      - name: Load cached Docker image
        run: |
          if [ -f /tmp/docker-cache/ollama-image.tar ]; then
            docker load < /tmp/docker-cache/ollama-image.tar
            echo "Loaded Ollama Docker image from cache"
          else
            echo "No cached Docker image found, will pull from registry"
            mkdir -p /tmp/docker-cache
            docker pull ollama/ollama:latest
            docker save ollama/ollama:latest > /tmp/docker-cache/ollama-image.tar
          fi
      
      - name: Run Ollama with Deepseek model
        run: |
          # Create Ollama models directory for cache persistence
          mkdir -p ~/.ollama/models
          # Run Ollama container with cache volume mount
          docker run -d --name ollama -p 11434:11434 -v ~/.ollama/models:/root/.ollama/models ollama/ollama
          echo "Waiting for Ollama to start..."
          sleep 10
          # Pull deepseek-coder model which is available in Ollama
          docker exec ollama ollama pull deepseek-coder
          echo "Deepseek model pulled successfully"
      
      - name: Query Deepseek model
        id: query-model
        run: |
          PROMPT="${{ github.event.inputs.prompt }}"
          echo "Sending prompt to Deepseek model: $PROMPT"
          
          RESPONSE=$(curl -s http://localhost:11434/api/generate -d '{
            "model": "deepseek-coder",
            "prompt": "'"$PROMPT"'",
            "stream": false
          }')
          
          # Extract the response content from the JSON
          CONTENT=$(echo $RESPONSE | jq -r '.response')
          
          # Use GitHub Actions output command to save the response
          echo "response<<EOF" >> $GITHUB_OUTPUT
          echo "$CONTENT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          echo "Response from Deepseek model saved"
      
      - name: Display model response
        run: |
          echo "Response from Deepseek model:"
          echo "${{ steps.query-model.outputs.response }}"
